{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from math import sqrt\n",
    "import  pylab as pl\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import hierarchical_portfolio\n",
    "from pyhrp.hrp import dist, linkage, tree, _hrp\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from cvxpy import cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_returns(x, y):\n",
    "    # save returns for each cluster in dictionary\n",
    "    # takes as input a dataframe containing the assets and corresponding cluster index (x) and a matrix of returns (y)\n",
    "    # returns a dictionary containing the returns for each cluster\n",
    "    x.columns = [\"Asset\",\"Cluster\"]\n",
    "    (unique, counts) = np.unique(x[\"Cluster\"], return_counts=True)\n",
    "    \n",
    "    ret_dict = {}\n",
    "    for i in unique: \n",
    "        lgc = x[\"Cluster\"] == i\n",
    "        ret_dict[i] = y.loc[:,lgc.values]\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_weights(x):\n",
    "    # computes equally weighted portfolio returns for each cluster\n",
    "    # takes a dictionary containing the cluster returns as input (output of cluster_returns)\n",
    "    # returns a dataframe containing the equally weighted portfolio returns\n",
    "    ret_ew = pd.DataFrame()\n",
    "\n",
    "    for i in x.keys(): \n",
    "        ret_ew = ret_ew.append(x[i].mean(axis = 1), ignore_index = True)\n",
    "\n",
    "    ret_ew = ret_ew.T\n",
    "    \n",
    "    return(ret_ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_portfolio(x, clust_type):\n",
    "    # computes optimal portfolio weights on 252 days rolling windows (daily portfolio rebalancing)\n",
    "    # takes dataframe of cluster returns as input (output of equal_weights)\n",
    "    # returns dataframe of optimal portfolio weights for each cluster\n",
    "    window = 252\n",
    "\n",
    "    #pesi_df = pd.DataFrame()\n",
    "    pesi_dict = {}\n",
    "    dates_dict = {}\n",
    "\n",
    "    for i in range( len(x) - window - 1 ):\n",
    "        \n",
    "        rets_rolling = x.iloc[i:(i+window),:] \n",
    "\n",
    "        if clust_type == \"hierarchical\":\n",
    "            \n",
    "            hrp = hierarchical_portfolio.HRPOpt(rets_rolling)\n",
    "            raw_weights = hrp.optimize()\n",
    "            weights_fin = hrp.clean_weights()\n",
    "            \n",
    "        else:\n",
    "            # expected returns and sample covariance \n",
    "            cluster_price = expected_returns.prices_from_returns(rets_rolling)\n",
    "            mu = expected_returns.mean_historical_return(cluster_price)\n",
    "            S = risk_models.sample_cov(cluster_price)\n",
    "            \n",
    "            try:\n",
    "                # maximum sharpe ratio (tangency) portfolio weights \n",
    "                ef = EfficientFrontier(mu, S)\n",
    "                raw_weights = ef.max_sharpe()\n",
    "                weights_fin = ef.clean_weights()\n",
    "\n",
    "            except:\n",
    "                ef = EfficientFrontier(mu, S, solver = 'CVXOPT')\n",
    "                raw_weights = ef.max_sharpe()\n",
    "                weights_fin = ef.clean_weights()\n",
    "\n",
    "        #pesi_df = pesi_df.append( pd.DataFrame(weights_fin.values()).T )\n",
    "        pesi_dict[i] = weights_fin.values()\n",
    "        dates_dict[i] = rets_rolling.index[-1] \n",
    "\n",
    "    #pesi_df.index = pd.Index(dates_dict.values())\n",
    "\n",
    "    pesi_df = pd.DataFrame(pesi_dict.values())\n",
    "    pesi_df.index = pd.Index(dates_dict.values())\n",
    "    pesi_df.columns = x.columns\n",
    "\n",
    "    return(pesi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_portfolio(x):\n",
    "    # performs static portfolio optimization\n",
    "    # takes dataframe of cluster returns as input (output of equal_weights)\n",
    "    # returns dictionary of optimal portfolio weights for each cluster\n",
    "\n",
    "    cluster_price = expected_returns.prices_from_returns(x)\n",
    "    mu = expected_returns.mean_historical_return(cluster_price)\n",
    "    S = risk_models.sample_cov(cluster_price)\n",
    "\n",
    "    ef = EfficientFrontier(mu, S)\n",
    "    raw_weights = ef.max_sharpe()\n",
    "    weights = ef.clean_weights()\n",
    "    \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_returns(pf_returns, d): \n",
    "    # computes cumulative returns\n",
    "    # takes as input a vector of portfolio returns\n",
    "    # returns dataframe of cumulative returns on test set\n",
    "    \n",
    "    # select correct dates\n",
    "    lgc = pf_returns.index.isin(d)\n",
    "    ret_fin = pf_returns.iloc[lgc]\n",
    "\n",
    "    # cumulative returns\n",
    "    cumulative = pd.DataFrame( 100*( (ret_fin + 1).cumprod() - 1 ) )\n",
    "    \n",
    "    return(cumulative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "Historical returns from about 200 stocks from Nasdaq, chosen randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2010,1,1)\n",
    "end = datetime(2020,12,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download symbols of all nasdaq components\n",
    "url=\"https://pkgstore.datahub.io/core/nasdaq-listings/nasdaq-listed_csv/data/7665719fb51081ba0bd834fde71ce822/nasdaq-listed_csv.csv\"\n",
    "s = requests.get(url).content\n",
    "companies = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "symbols = companies['Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 500 assets\n",
    "random.seed(123)\n",
    "Symbols = random.sample(symbols, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download stock prices\n",
    "stock_final = pd.DataFrame()\n",
    "\n",
    "for i in Symbols:  \n",
    "    try:\n",
    "        stock = []\n",
    "        stock = yf.download(i,start=start, end=end, progress=False)\n",
    "        \n",
    "        if len(stock) == 0:\n",
    "            None\n",
    "        else:\n",
    "            stock['Name']= i\n",
    "            stock_final = stock_final.append(stock,sort=False)\n",
    "    except Exception:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of closing prices \n",
    "close = stock_final[[\"Close\", \"Name\"]]\n",
    "close_wide = close.pivot_table(index=\"Date\", columns='Name', values='Close')\n",
    "stock_price = close_wide.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stocks = len(stock_price.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock returns\n",
    "returns = stock_price.pct_change().iloc[1:]\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import returns\n",
    "returns_imp = pd.read_pickle('returns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import 3 month tbill\n",
    "tbills = pd.read_pickle('DTB3.pkl')\n",
    "\n",
    "# annualized tbill \n",
    "annualized = []\n",
    "\n",
    "for i in tbills['DTB3']:\n",
    "    try:\n",
    "       annualized.append((1 + float(i))**(1/252) - 1)\n",
    "    except ValueError:\n",
    "       annualized.append(0.0)\n",
    "\n",
    "tbills['DTB3A'] = annualized\n",
    "\n",
    "tbills.index = pd.to_datetime(tbills.index)\n",
    "tbills.index.names = ['Date']\n",
    "merged = pd.merge(tbills, returns_imp, on='Date')\n",
    "tbill_ann = pd.DataFrame( merged[\"DTB3A\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual mean returns and variances \n",
    "mean_ret = returns_imp.mean() * 252\n",
    "var_ret = returns_imp.std() * sqrt(252)\n",
    "rets_df = pd.concat([mean_ret, var_ret], axis = 1)\n",
    "rets_df.columns = [\"Returns\",\"Variance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect outliers\n",
    "X = rets_df.values \n",
    "pl.scatter(X[:,0],X[:,1])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "xx = pd.DataFrame(mean_ret)\n",
    "xx_sorted = xx.sort_values(by = 0)[-2:].index\n",
    "outliers = list(xx_sorted)\n",
    "\n",
    "returns = returns_imp.drop(outliers, 1)\n",
    "rets_df2 = rets_df.drop(outliers)\n",
    "\n",
    "X = rets_df2.values \n",
    "pl.scatter(X[:,0],X[:,1])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test set \n",
    "cutoff = \"2019-12-31\"\n",
    "\n",
    "ret_train = returns[returns.index <= cutoff]\n",
    "\n",
    "ret_test = returns[returns.index > (datetime.strptime(cutoff, '%Y-%m-%d') -  relativedelta(years=1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "\n",
    "The k-means algorithm divides a set of $N$ samples $X$ into $C$ disjoint clusters, each described by the mean $\\mu_i$ \n",
    "of the samples in the cluster (the centroid). The K-means algorithm aims to choose centroids that minimise the within-cluster sum-of-squares:\n",
    "$$\n",
    "\\sum_{i=0}^{n} \\underset{\\mu_i \\in C}{min} (| x_i - \\mu_i |)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual mean returns and variances \n",
    "mean_ret = ret_train.mean() * 252\n",
    "var_ret = ret_train.std() * sqrt(252)\n",
    "rets_df = pd.concat([mean_ret, var_ret], axis = 1)\n",
    "rets_df.columns = [\"Returns\",\"Variance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select optimal number of clusters by minimizing SSE\n",
    "X =  rets_df.values \n",
    "sse = []\n",
    "\n",
    "random.seed(123)\n",
    "for k in range(2,15):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = k)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    sse.append(kmeans.inertia_) #SSE for each cluster\n",
    "    \n",
    "pl.plot(range(2,15), sse)\n",
    "pl.title(\"Elbow Curve\")\n",
    "pl.xlabel('nr clusters')\n",
    "pl.ylabel('SSE')\n",
    "pl.axvline(x=5, c = \"k\", linestyle='dashed')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit k-means with 5 clusters\n",
    "X = rets_df.values \n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters = n_clusters).fit(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "pl.scatter(X[:,0],X[:,1], c = kmeans.labels_, cmap = \"rainbow\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of elements in each cluster\n",
    "cluster_idx = np.array(kmeans.labels_)\n",
    "(unique, counts) = np.unique(cluster_idx, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster number for each asset\n",
    "asset = pd.DataFrame(rets_df.index)\n",
    "cluster_list = pd.concat([asset, pd.DataFrame(cluster_idx)],axis = 1)\n",
    "cluster_list.columns = [\"Asset\",\"Cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling portfolio optimization K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save return series for each cluster in dictionary\n",
    "ret_dict = cluster_returns(x = cluster_list, y = ret_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute equally weighted portfolio returns for each cluster\n",
    "ret_ew = equal_weights(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal portfolio weights on 252 days rolling windows (daily portfolio rebalancing)\n",
    "pesi_rol = rolling_portfolio(ret_ew, clust_type = \"partitional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure return dates match weight dates \n",
    "lgc = ret_ew.index.isin(pesi_rol.index)\n",
    "ret_subs = ret_ew.iloc[lgc, :]\n",
    "\n",
    "# select dates for portfolio backtest\n",
    "dates_backtest = ret_subs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "kmeans_rets = (ret_subs*pesi_rol).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "kmeans_rol = cumulative_returns(pf_returns = kmeans_rets, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(kmeans_rol)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static portfolio optimization K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static portfolio optimization\n",
    "pesi_static = static_portfolio(ret_ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "kmeans_rets_static = (ret_ew*list(pesi_static.values())).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "kmeans_static = cumulative_returns(pf_returns = kmeans_rets_static, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(kmeans_static)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate portfolio weights for each asset\n",
    "# kmeans_w = pd.DataFrame()\n",
    "\n",
    "# names_assets = pd.Index([])\n",
    "\n",
    "# for i in pesi_static.keys():\n",
    "    \n",
    "#     w = pesi_static[i]\n",
    "#     pesi = [w*1/len(ret_dict[i].columns)]*len(ret_dict[i].columns)\n",
    "#     kmeans_w = kmeans_w.append( pd.DataFrame(pesi) )\n",
    "#     names_assets = names_assets.union( ret_dict[i].columns )\n",
    "\n",
    "# kmeans_w.index = names_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excess return from daily rebalancing\n",
    "extraret = kmeans_rol - kmeans_static\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(extraret)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Excess Return\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical risk parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrp clusters (pyhrp package)\n",
    "cov, cor = ret_train.cov(), ret_train.corr()\n",
    "links = linkage(dist(cor.values), method='ward')\n",
    "node = tree(links)\n",
    "\n",
    "#rootcluster = _hrp(node, cov)\n",
    "#weights_hrp = rootcluster.weights\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax = dendrogram(links, orientation=\"top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static portfolio optimization HRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static portfolio weights (pypfopt package)\n",
    "hrp = hierarchical_portfolio.HRPOpt(ret_train)\n",
    "raw_weights = hrp.optimize()\n",
    "weights_hrp = hrp.clean_weights()\n",
    "# cluster = hrp.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "hrp_rets_static = (ret_test*list(weights_hrp.values())).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "hrp_static = cumulative_returns(pf_returns = hrp_rets_static, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(hrp_static)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling portfolio optimization HRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hrp_pesirol = rolling_portfolio(ret_test, clust_type = \"hierarchical\")\n",
    "\n",
    "#hrp_pesirol.to_pickle(\"./pesi_rolling_hrp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rolling hrp weights\n",
    "hrp_pesirol = pd.read_pickle(\"./pesi_rolling_hrp.pkl\")\n",
    "\n",
    "hrp_pesirol.columns = ret_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "hrp_rets_rol = (ret_test*hrp_pesirol).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "hrp_rol = cumulative_returns(pf_returns = hrp_rets_rol, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(hrp_rol)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling tangency portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tangency_rol = rolling_portfolio(ret_test, clust_type = \"none\")\n",
    "\n",
    "#tangency_rol.to_pickle(\"./pesi_rolling_tangency.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rolling tangency weights\n",
    "tan_pesirol = pd.read_pickle(\"./pesi_rolling_tangency.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvxpy.installed_solvers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "tan_rets_rol = (ret_test*tan_pesirol).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "tan_rol = cumulative_returns(pf_returns = tan_rets_rol, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(tan_rol)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static tangency portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio weights\n",
    "cluster_price = expected_returns.prices_from_returns(ret_test)\n",
    "mu = expected_returns.mean_historical_return(cluster_price)\n",
    "S = risk_models.sample_cov(cluster_price)\n",
    "ef = EfficientFrontier(mu, S)\n",
    "raw_weights = ef.max_sharpe()\n",
    "tan_pesi_static = ef.clean_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "tan_rets_rol = (ret_test*tan_pesi_static).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "tan_rol = cumulative_returns(pf_returns = tan_rets_rol, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(tan_rol)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equally weighted portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_rets = ret_test.mean(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "ew_rol = cumulative_returns(pf_returns = ew_rets, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(ew_rol)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
