{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from math import sqrt\n",
    "import  pylab as pl\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import hierarchical_portfolio\n",
    "from pyhrp.hrp import dist, linkage, tree, _hrp\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from cvxpy import cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class bounded k-means\n",
    "class ClustersUtils:\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_dist_array(cls, X: np.array) -> np.array:\n",
    "        dist_array = np.array([[np.linalg.norm(x1 - x2)\n",
    "                                for x1 in X]\n",
    "                               for x2 in X])\n",
    "        return dist_array\n",
    "    \n",
    "    @classmethod\n",
    "    def scatter_plot(cls, X: np.array, clusters_in_idxs: [[int]], centroid_idxs: [int] = None):\n",
    "        \"\"\"Only plots first two dimensions\"\"\"\n",
    "        x, y = list(zip(*[[X[c_idx][0], X[c_idx][1]]\n",
    "                          for one_cluster_in_idxs in clusters_in_idxs\n",
    "                          for c_idx in one_cluster_in_idxs]))\n",
    "        c = [color_idx\n",
    "             for color_idx, one_cluster_in_idxs in enumerate(clusters_in_idxs)\n",
    "             for _ in one_cluster_in_idxs]\n",
    "        df = pd.DataFrame({'x': x, 'y': y, 'c': c})\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        for color_idx, cluster_in_idxs in enumerate(clusters_in_idxs):\n",
    "            df_temp = df[df['c'].isin([color_idx])]\n",
    "            plt.plot(df_temp['x'].tolist(), df_temp['y'].tolist(), 'o', label=color_idx, markersize=5)\n",
    "\n",
    "        if centroid_idxs is not None:\n",
    "            x_c, y_c = list(zip(*[[X[c_idx][0], X[c_idx][1]]\n",
    "                                  for c_idx in centroid_idxs]))\n",
    "            plt.plot(x_c, y_c, 'o', color='black', markersize=3)\n",
    "\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.show()\n",
    "        \n",
    "    @classmethod\n",
    "    def plot_families_on_map(cls, points_coords_lng_lat: [[float]], family_size: np.array) -> None:\n",
    "        features = []\n",
    "        color = \"#251782\"\n",
    "        for lng_lat, s in zip(points_coords_lng_lat, family_size):\n",
    "            p = Feature(geometry=GeojsonPoint(lng_lat), \n",
    "                        properties={\"marker-symbol\": int(s), \"marker-color\": color})\n",
    "            features += [p]\n",
    "\n",
    "        feature_collection = FeatureCollection(features=features)\n",
    "        geojsonio.display(json.dumps(feature_collection));\n",
    "        \n",
    "    @classmethod\n",
    "    def plot_clustering_on_map(cls, clusters_in_idxs: [[int]], \n",
    "                               points_coords_lng_lat: [[float]], family_size: np.array) -> None:\n",
    "        features = []\n",
    "        for cluster_idxs in clusters_in_idxs:\n",
    "            color = \"#\" + ''.join(random.choices('0123456789abcdef', k=6))\n",
    "            for idx in cluster_idxs:\n",
    "                properties = {\"marker-symbol\": int(family_size[idx]), \"marker-color\": color}\n",
    "                p = Feature(geometry=GeojsonPoint(points_coords_lng_lat[idx]), properties=properties)\n",
    "                features += [p]\n",
    "\n",
    "        feature_collection = FeatureCollection(features=features)\n",
    "        geojsonio.display(json.dumps(feature_collection));\n",
    "        \n",
    "class BoundedKMeansClustering:\n",
    "    def __init__(self, n_clusters: int, max_cluster_size: int, n_iter: int = 10, n_init: int = 10, plot_every_iteration=False):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_cluster_size = max_cluster_size\n",
    "        self.n_iter = n_iter\n",
    "        self.n_init = n_init\n",
    "        self.plot_every_iteration = plot_every_iteration\n",
    "\n",
    "        self.n_points = None\n",
    "\n",
    "    def fit(self, X: np.array, weights: np.array, dist_array: np.array = None) -> [float, [[int]]]:\n",
    "        self.n_points = X.shape[0]\n",
    "        dist_array = dist_array if dist_array is not None else ClustersUtils.compute_dist_array(X)\n",
    "\n",
    "        costs, clusters = zip(*[self.fit_one_iteration(X, weights, dist_array) for _ in range(self.n_init)])\n",
    "\n",
    "        if all(np.isnan(np.array(costs))):\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        best_idx = np.nanargmin(costs)\n",
    "        best_cost = costs[best_idx]\n",
    "        best_clusters = clusters[best_idx]\n",
    "\n",
    "        self.n_points = None\n",
    "        return best_cost, best_clusters\n",
    "\n",
    "    def fit_one_iteration(self, X: np.array, weights: np.array, dist_array: np.array) -> [float, [[int]]]:\n",
    "        # inspired by https://core.ac.uk/download/pdf/61217069.pdf\n",
    "\n",
    "        try:\n",
    "            clusters_in_idxs = self._initialize_clusters(weights, dist_array)\n",
    "            best_clusters = clusters_in_idxs\n",
    "            best_cost = self._get_maximal_mean_dist_in_clusters(dist_array, clusters_in_idxs)\n",
    "\n",
    "            for i in range(self.n_iter):\n",
    "                clusters_in_idxs, maximal_mean_dist_in_clusters = self._optimize_clusters(X, weights, dist_array,\n",
    "                                                                                          clusters_in_idxs)\n",
    "                if clusters_in_idxs == best_clusters:\n",
    "                    # print(f\"Reached a local optimum after {i} iterations.\")\n",
    "                    break\n",
    "                \n",
    "                if maximal_mean_dist_in_clusters < best_cost:\n",
    "                    best_cost = maximal_mean_dist_in_clusters\n",
    "                    best_clusters = clusters_in_idxs\n",
    "                    \n",
    "            if self.plot_every_iteration:\n",
    "                ClustersUtils.scatter_plot(X, best_clusters)\n",
    "\n",
    "        except ValueError:\n",
    "            best_cost, best_clusters = np.nan, np.nan\n",
    "\n",
    "        return best_cost, best_clusters\n",
    "\n",
    "    def _initialize_clusters(self, weights: np.array, dist_array: np.array) -> [[int]]:\n",
    "        centroid_idxs = random.sample(range(self.n_points), self.n_clusters)\n",
    "        clusters_in_idxs = self._assign_points_to_clusters(weights, dist_array, centroid_idxs)\n",
    "        return clusters_in_idxs\n",
    "\n",
    "    def _assign_points_to_clusters(self, weights: np.array, dist_array: np.array, centroid_idxs: [int]) -> [[int]]:\n",
    "        clusters_in_idxs = [[c_idx] for c_idx in centroid_idxs]\n",
    "        cluster_weights = np.array([weights[c_idx] for c_idx in centroid_idxs])\n",
    "\n",
    "        sorted_points_idxs_by_weights = [i for i in np.argsort(-weights,  axis=0) if i not in centroid_idxs]\n",
    "        for p_idx in sorted_points_idxs_by_weights:\n",
    "            is_assigned = False\n",
    "            sorted_cluster_idxs_by_dist = np.argsort(dist_array[p_idx][centroid_idxs])\n",
    "            for c_idx in sorted_cluster_idxs_by_dist:\n",
    "                if cluster_weights[c_idx] + weights[p_idx] <= self.max_cluster_size:\n",
    "                    clusters_in_idxs[c_idx].append(p_idx)\n",
    "                    cluster_weights[c_idx] += weights[p_idx]\n",
    "                    is_assigned = True\n",
    "                    break\n",
    "            if not is_assigned:\n",
    "                raise ValueError(\n",
    "                    f\"Point {p_idx} could not be assigned. Try with more than {self.n_clusters} clusters. \"\n",
    "                    f\"Current_clusters in idxs: {clusters_in_idxs}\")\n",
    "\n",
    "        return clusters_in_idxs\n",
    "\n",
    "    def _get_maximal_mean_dist_in_clusters(self, dist_array: np.array, clusters_in_idxs: [[int]]):\n",
    "        mean_dist_in_all_clusters = [self._get_mean_dist_in_cluster(dist_array, one_cluster_in_idxs)\n",
    "                                     for one_cluster_in_idxs in clusters_in_idxs]\n",
    "        maximal_mean_dist_in_clusters = max(mean_dist_in_all_clusters)\n",
    "        return maximal_mean_dist_in_clusters\n",
    "\n",
    "    def _get_mean_dist_in_cluster(self, dist_array: np.array, cluster_in_idxs: [[int]]) -> float:\n",
    "        cluster_sub_dist_array = dist_array[cluster_in_idxs, :][:, cluster_in_idxs]\n",
    "        cluster_sub_dist_array_triu = np.triu(cluster_sub_dist_array)\n",
    "        cluster_sub_dist_array_triu[cluster_sub_dist_array_triu == 0] = np.nan\n",
    "        mean_dist_in_cluster = np.nanmean(cluster_sub_dist_array)\n",
    "        return mean_dist_in_cluster\n",
    "\n",
    "    def _optimize_clusters(self, X: np.array, weights: np.array, dist_array: np.array, clusters_in_idxs: [[int]]):\n",
    "        centroid_idxs = self._update_centroids(X, clusters_in_idxs)\n",
    "        clusters_in_idxs = self._assign_points_to_clusters(weights, dist_array, centroid_idxs)\n",
    "        maximal_mean_dist_in_clusters = self._get_maximal_mean_dist_in_clusters(dist_array, clusters_in_idxs)        \n",
    "        return clusters_in_idxs, maximal_mean_dist_in_clusters\n",
    "\n",
    "    def _update_centroids(self, X: np.array, clusters_in_idxs: [[int]]) -> [int]:\n",
    "        updated_centroid_idxs = [self._update_centroid_for_one_cluster(X, one_cluster_in_idxs)\n",
    "                                 for one_cluster_in_idxs in clusters_in_idxs]\n",
    "        return updated_centroid_idxs\n",
    "\n",
    "    def _update_centroid_for_one_cluster(self, X: np.array, cluster_in_idxs: [int]) -> int:\n",
    "        center = np.mean(X[cluster_in_idxs], axis=0)\n",
    "        closest_point_idx_in_cluster = np.argmin(np.linalg.norm(X[cluster_in_idxs] - center, axis=1))\n",
    "        closest_point_idx = cluster_in_idxs[closest_point_idx_in_cluster]\n",
    "        return closest_point_idx\n",
    "    \n",
    "class BoundedClustering:\n",
    "    def __init__(self, max_cluster_size: int, n_iter: int = 10, n_init: int = 10, n_k_to_try: int = 10, \n",
    "                 plot_every_iteration: bool = False, plot_edge_values: bool = False):\n",
    "        self.max_cluster_size = max_cluster_size\n",
    "        self.n_iter = n_iter\n",
    "        self.n_init = n_init\n",
    "        self.n_k_to_try = n_k_to_try\n",
    "        self.plot_every_iteration = plot_every_iteration\n",
    "        self.plot_edge_values = plot_edge_values\n",
    "\n",
    "    def fit(self, X: np.array, weights: np.array) -> [float, [[int]]]:\n",
    "        min_n_clusters = int(np.ceil(sum(weights) / self.max_cluster_size))\n",
    "        max_n_clusters = min([min_n_clusters + self.n_k_to_try, len(weights)])\n",
    "        all_k = list(range(min_n_clusters, max_n_clusters + 1))\n",
    "\n",
    "        dist_array = ClustersUtils.compute_dist_array(X)\n",
    "        costs, clusters = zip(*[self._fit_k(k, X, weights, dist_array) for k in all_k])\n",
    "\n",
    "        if np.isnan(np.array(costs)).all():\n",
    "            raise NotEnoughClustersException(f\"Could not produce any clustering with range {all_k}.\")\n",
    "\n",
    "        valid_k_indices, all_points = zip(*[[i, np.array([k, cost])]\n",
    "                                            for i, (k, cost) in enumerate(zip(all_k, costs))\n",
    "                                            if not np.isnan(cost)])\n",
    "        if len(valid_k_indices) <= 2:\n",
    "            i = valid_k_indices[0]\n",
    "            return all_k[i], clusters[i]\n",
    "\n",
    "        p_left, p_right = all_points[0], all_points[-1]\n",
    "        all_distances_from_line = [self._dist_from_line(p_left, p_right, p) for p in all_points]\n",
    "        \n",
    "        best_k_idx_in_all_points = int(np.argmax(all_distances_from_line))\n",
    "        self._plot_elbow(all_points, best_k_idx_in_all_points)\n",
    "        \n",
    "        if self.plot_edge_values:\n",
    "            ClustersUtils.scatter_plot(X, clusters[valid_k_indices[0]])\n",
    "            ClustersUtils.scatter_plot(X, clusters[valid_k_indices[-1]])\n",
    "        \n",
    "        best_k_idx_in_all_ks = valid_k_indices[best_k_idx_in_all_points]\n",
    "        best_n_clusters, best_clusters = all_k[best_k_idx_in_all_ks], clusters[best_k_idx_in_all_ks]\n",
    "        return best_n_clusters, best_clusters\n",
    "    \n",
    "    def _fit_k(self, k: int, X: np.array, weights: np.array, dist_array: np.array = None) -> [float, [[int]]]:\n",
    "        cluster_maker = BoundedKMeansClustering(k, self.max_cluster_size, self.n_iter, self.n_init)\n",
    "        best_cost, best_clusters = cluster_maker.fit(X, weights, dist_array)\n",
    "        return best_cost, best_clusters\n",
    "    \n",
    "    def _dist_from_line(self, p_left: np.array, p_right: np.array, p: np.array) -> float:\n",
    "        # minus sign because we expect the point to be below the line\n",
    "        d = -np.cross(p_right - p_left, p - p_left) / np.linalg.norm(p_right - p_left)\n",
    "        return d\n",
    "    \n",
    "    def _plot_elbow(self, all_points, best_k_idx_in_all_points) -> None:\n",
    "        points_list = np.array(all_points).T.tolist()\n",
    "        plt.plot(points_list[0], points_list[1]);\n",
    "        plt.plot([points_list[0][0], points_list[0][-1]], [points_list[1][0], points_list[1][-1]]);\n",
    "        plt.scatter(all_points[best_k_idx_in_all_points][0], all_points[best_k_idx_in_all_points][1]);\n",
    "        plt.xlabel('Number of clusters');\n",
    "        plt.ylabel('Cost');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_returns(x, y):\n",
    "    # save returns for each cluster in dictionary\n",
    "    # takes as input a dataframe containing the assets and corresponding cluster index (x) and a matrix of returns (y)\n",
    "    # returns a dictionary containing the returns for each cluster\n",
    "    x.columns = [\"Asset\",\"Cluster\"]\n",
    "    (unique, counts) = np.unique(x[\"Cluster\"], return_counts=True)\n",
    "    \n",
    "    ret_dict = {}\n",
    "    for i in unique: \n",
    "        lgc = x[\"Cluster\"] == i\n",
    "        ret_dict[i] = y.loc[:,lgc.values]\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_weights(x):\n",
    "    # computes equally weighted portfolio returns for each cluster\n",
    "    # takes a dictionary containing the cluster returns as input (output of cluster_returns)\n",
    "    # returns a dataframe containing the equally weighted portfolio returns\n",
    "    ret_ew = pd.DataFrame()\n",
    "\n",
    "    for i in x.keys(): \n",
    "        ret_ew = ret_ew.append(x[i].mean(axis = 1), ignore_index = True)\n",
    "\n",
    "    ret_ew = ret_ew.T\n",
    "    \n",
    "    return(ret_ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_portfolio(x, clust_type):\n",
    "    # computes optimal portfolio weights on 252 days rolling windows (daily portfolio rebalancing)\n",
    "    # takes dataframe of cluster returns as input (output of equal_weights)\n",
    "    # returns dataframe of optimal portfolio weights for each cluster\n",
    "    window = 252\n",
    "\n",
    "    #pesi_df = pd.DataFrame()\n",
    "    pesi_dict = {}\n",
    "    dates_dict = {}\n",
    "\n",
    "    for i in range( len(x) - window - 1 ):\n",
    "        \n",
    "        rets_rolling = x.iloc[i:(i+window),:] \n",
    "\n",
    "        if clust_type == \"hierarchical\":\n",
    "            \n",
    "            hrp = hierarchical_portfolio.HRPOpt(rets_rolling)\n",
    "            raw_weights = hrp.optimize()\n",
    "            weights_fin = hrp.clean_weights()\n",
    "            \n",
    "        else:\n",
    "            # expected returns and sample covariance \n",
    "            cluster_price = expected_returns.prices_from_returns(rets_rolling)\n",
    "            mu = expected_returns.mean_historical_return(cluster_price)\n",
    "            S = risk_models.sample_cov(cluster_price)\n",
    "            \n",
    "            try:\n",
    "                # maximum sharpe ratio (tangency) portfolio weights \n",
    "                ef = EfficientFrontier(mu, S)\n",
    "                raw_weights = ef.max_sharpe()\n",
    "                weights_fin = ef.clean_weights()\n",
    "\n",
    "            except:\n",
    "                ef = EfficientFrontier(mu, S, solver = 'CVXOPT')\n",
    "                raw_weights = ef.max_sharpe()\n",
    "                weights_fin = ef.clean_weights()\n",
    "\n",
    "        #pesi_df = pesi_df.append( pd.DataFrame(weights_fin.values()).T )\n",
    "        pesi_dict[i] = weights_fin.values()\n",
    "        dates_dict[i] = rets_rolling.index[-1] \n",
    "\n",
    "    #pesi_df.index = pd.Index(dates_dict.values())\n",
    "\n",
    "    pesi_df = pd.DataFrame(pesi_dict.values())\n",
    "    pesi_df.index = pd.Index(dates_dict.values())\n",
    "    pesi_df.columns = x.columns\n",
    "\n",
    "    return(pesi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_portfolio(x):\n",
    "    # performs static portfolio optimization\n",
    "    # takes dataframe of cluster returns as input (output of equal_weights)\n",
    "    # returns dictionary of optimal portfolio weights for each cluster\n",
    "\n",
    "    cluster_price = expected_returns.prices_from_returns(x)\n",
    "    mu = expected_returns.mean_historical_return(cluster_price)\n",
    "    S = risk_models.sample_cov(cluster_price)\n",
    "\n",
    "    ef = EfficientFrontier(mu, S)\n",
    "    raw_weights = ef.max_sharpe()\n",
    "    weights = ef.clean_weights()\n",
    "    \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_returns(pf_returns, d): \n",
    "    # computes cumulative returns\n",
    "    # takes as input a vector of portfolio returns\n",
    "    # returns dataframe of cumulative returns on test set\n",
    "    \n",
    "    # select correct dates\n",
    "    lgc = pf_returns.index.isin(d)\n",
    "    ret_fin = pf_returns.iloc[lgc]\n",
    "\n",
    "    # cumulative returns\n",
    "    cumulative = pd.DataFrame( 100*( (ret_fin + 1).cumprod() - 1 ) )\n",
    "    \n",
    "    return(cumulative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "Historical returns from about 200 stocks from Nasdaq, chosen randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2010,1,1)\n",
    "end = datetime(2020,12,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download symbols of all nasdaq components\n",
    "url=\"https://pkgstore.datahub.io/core/nasdaq-listings/nasdaq-listed_csv/data/7665719fb51081ba0bd834fde71ce822/nasdaq-listed_csv.csv\"\n",
    "s = requests.get(url).content\n",
    "companies = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "symbols = companies['Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 500 assets\n",
    "random.seed(123)\n",
    "Symbols = random.sample(symbols, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download stock prices\n",
    "stock_final = pd.DataFrame()\n",
    "\n",
    "for i in Symbols:  \n",
    "    try:\n",
    "        stock = []\n",
    "        stock = yf.download(i,start=start, end=end, progress=False)\n",
    "        \n",
    "        if len(stock) == 0:\n",
    "            None\n",
    "        else:\n",
    "            stock['Name']= i\n",
    "            stock_final = stock_final.append(stock,sort=False)\n",
    "    except Exception:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of closing prices \n",
    "close = stock_final[[\"Close\", \"Name\"]]\n",
    "close_wide = close.pivot_table(index=\"Date\", columns='Name', values='Close')\n",
    "stock_price = close_wide.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stocks = len(stock_price.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock returns\n",
    "returns = stock_price.pct_change().iloc[1:]\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import returns\n",
    "returns_imp = pd.read_pickle('returns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import 3 month tbill\n",
    "tbills = pd.read_pickle('DTB3.pkl')\n",
    "\n",
    "# annualized tbill \n",
    "annualized = []\n",
    "\n",
    "for i in tbills['DTB3']:\n",
    "    try:\n",
    "       annualized.append((1 + float(i))**(1/252) - 1)\n",
    "    except ValueError:\n",
    "       annualized.append(0.0)\n",
    "\n",
    "tbills['DTB3A'] = annualized\n",
    "\n",
    "tbills.index = pd.to_datetime(tbills.index)\n",
    "tbills.index.names = ['Date']\n",
    "merged = pd.merge(tbills, returns_imp, on='Date')\n",
    "tbill_ann = pd.DataFrame( merged[\"DTB3A\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual mean returns and variances \n",
    "mean_ret = returns_imp.mean() * 252\n",
    "var_ret = returns_imp.std() * sqrt(252)\n",
    "rets_df = pd.concat([mean_ret, var_ret], axis = 1)\n",
    "rets_df.columns = [\"Returns\",\"Variance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect outliers\n",
    "X = rets_df.values \n",
    "pl.scatter(X[:,0],X[:,1])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "xx = pd.DataFrame(mean_ret)\n",
    "xx_sorted = xx.sort_values(by = 0)[-2:].index\n",
    "outliers = list(xx_sorted)\n",
    "\n",
    "returns = returns_imp.drop(outliers, 1)\n",
    "rets_df2 = rets_df.drop(outliers)\n",
    "\n",
    "X = rets_df2.values \n",
    "pl.scatter(X[:,0],X[:,1])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test set \n",
    "cutoff = \"2019-12-31\"\n",
    "\n",
    "ret_train = returns[returns.index <= cutoff]\n",
    "\n",
    "ret_test = returns[returns.index > (datetime.strptime(cutoff, '%Y-%m-%d') -  relativedelta(years=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual mean returns and variances \n",
    "mean_ret = ret_train.mean() * 252\n",
    "var_ret = ret_train.std() * sqrt(252)\n",
    "rets_df = pd.concat([mean_ret, var_ret], axis = 1)\n",
    "rets_df.columns = [\"Returns\",\"Variance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "\n",
    "The k-means algorithm divides a set of $N$ samples $X$ into $C$ disjoint clusters, each described by the mean $\\mu_i$ \n",
    "of the samples in the cluster (the centroid). The K-means algorithm aims to choose centroids that minimise the within-cluster sum-of-squares:\n",
    "$$\n",
    "\\sum_{i=0}^{n} \\underset{\\mu_i \\in C}{min} (| x_i - \\mu_i |)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select optimal number of clusters by minimizing SSE\n",
    "X =  rets_df.values \n",
    "sse = []\n",
    "\n",
    "random.seed(123)\n",
    "for k in range(2,15):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = k)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    sse.append(kmeans.inertia_) #SSE for each cluster\n",
    "    \n",
    "pl.plot(range(2,15), sse)\n",
    "pl.title(\"Elbow Curve\")\n",
    "pl.xlabel('nr clusters')\n",
    "pl.ylabel('SSE')\n",
    "pl.axvline(x=5, c = \"k\", linestyle='dashed')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit k-means with 5 clusters\n",
    "X = rets_df.values \n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters = n_clusters).fit(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "pl.scatter(X[:,0],X[:,1], c = kmeans.labels_, cmap = \"rainbow\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of elements in each cluster\n",
    "cluster_idx = np.array(kmeans.labels_)\n",
    "(unique, counts) = np.unique(cluster_idx, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster number for each asset\n",
    "asset = pd.DataFrame(rets_df.index)\n",
    "cluster_list = pd.concat([asset, pd.DataFrame(cluster_idx)],axis = 1)\n",
    "cluster_list.columns = [\"Asset\",\"Cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling portfolio optimization K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save return series for each cluster in dictionary\n",
    "ret_dict = cluster_returns(x = cluster_list, y = ret_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute equally weighted portfolio returns for each cluster\n",
    "ret_ew = equal_weights(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal portfolio weights on 252 days rolling windows (daily portfolio rebalancing)\n",
    "pesi_rol = rolling_portfolio(ret_ew, clust_type = \"partitional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure return dates match weight dates \n",
    "lgc = ret_ew.index.isin(pesi_rol.index)\n",
    "ret_subs = ret_ew.iloc[lgc, :]\n",
    "\n",
    "# select dates for portfolio backtest\n",
    "dates_backtest = ret_subs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "kmeans_rets = (ret_subs*pesi_rol).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "kmeans_rol = cumulative_returns(pf_returns = kmeans_rets, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(kmeans_rol)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.legend (['Rolling portfolio optimization K-means'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static portfolio optimization K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static portfolio optimization\n",
    "pesi_static = static_portfolio(ret_ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "kmeans_rets_static = (ret_ew*list(pesi_static.values())).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "kmeans_static = cumulative_returns(pf_returns = kmeans_rets_static, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(kmeans_static)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.legend (['Static portfolio optimization K-means'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate portfolio weights for each asset\n",
    "# kmeans_w = pd.DataFrame()\n",
    "\n",
    "# names_assets = pd.Index([])\n",
    "\n",
    "# for i in pesi_static.keys():\n",
    "    \n",
    "#     w = pesi_static[i]\n",
    "#     pesi = [w*1/len(ret_dict[i].columns)]*len(ret_dict[i].columns)\n",
    "#     kmeans_w = kmeans_w.append( pd.DataFrame(pesi) )\n",
    "#     names_assets = names_assets.union( ret_dict[i].columns )\n",
    "\n",
    "# kmeans_w.index = names_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excess return from daily rebalancing\n",
    "extraret_km = kmeans_rol - kmeans_static\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(extraret_km)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Excess Return\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounded K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rets_df.values\n",
    "\n",
    "n_clusters = 5\n",
    "n_iter = 10\n",
    "weights = np.ones(len(X)) # weight of individual observations set to one \n",
    "max_cluster_size = (sum(weights)// n_clusters) + n_clusters\n",
    "\n",
    "random.seed(123)\n",
    "cluster_maker = BoundedClustering(max_cluster_size, n_iter) \n",
    "best_k, best_clusters = cluster_maker.fit(X, weights)\n",
    "print(f\"Best partition was found for {best_k} clusters\")\n",
    "print(f\"Total weight = {sum(weights)}\")\n",
    "print(f\"Max_cluster_size = {max_cluster_size}\")\n",
    "print(f\"Clusters weights: {dict((i, sum(weights[c])) for i, c in enumerate(best_clusters))}\")\n",
    "ClustersUtils.scatter_plot(X, best_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_bkm = {}\n",
    "for clu, assets in enumerate (best_clusters):\n",
    "    dict_bkm[clu] = assets\n",
    "\n",
    "cluster_array = np.zeros(len(X))\n",
    "\n",
    "for k, it in dict_bkm.items():\n",
    "    for i in it:\n",
    "        cluster_array[i] = k\n",
    "\n",
    "asset = pd.DataFrame(rets_df.index)\n",
    "cluster_bkm = pd.concat([asset, pd.DataFrame(cluster_array)], axis = 1)\n",
    "cluster_bkm.columns = ['assets', 'clusters bkm']\n",
    "cluster_bkm['clusters bkm'] = cluster_bkm['clusters bkm'].astype(int)\n",
    "cluster_bkm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling portfolio Bounded K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_dict = cluster_returns(x = cluster_bkm, y = ret_test)\n",
    "ret_ew = equal_weights(ret_dict)\n",
    "\n",
    "pesi_rol = rolling_portfolio(ret_ew, clust_type = \"partitional\")\n",
    "\n",
    "# make sure return dates match weight dates \n",
    "lgc = ret_ew.index.isin(pesi_rol.index)\n",
    "ret_subs = ret_ew.iloc[lgc, :]\n",
    "\n",
    "# select dates for portfolio backtest\n",
    "dates_backtest = ret_subs.index\n",
    "\n",
    "# portfolio returns\n",
    "bkm_rets = (ret_subs*pesi_rol).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "bkm_rol = cumulative_returns(pf_returns = bkm_rets, d = dates_backtest)\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(bkm_rol)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "plt.legend (['Rolling portfolio Bounded K-means'])\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Portfolio Optimization Bounded K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static portfolio optimization\n",
    "pesi_static = static_portfolio(ret_ew)\n",
    "\n",
    "# portfolio returns\n",
    "bkm_rets_static = (ret_ew*list(pesi_static.values())).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "bkm_static = cumulative_returns(pf_returns = bkm_rets_static, d = dates_backtest)\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(bkm_static)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.legend (['Static portfolio Bounded K-means'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate portfolio weights for each asset\n",
    "# kmeans_w = pd.DataFrame()\n",
    "\n",
    "# names_assets = pd.Index([])\n",
    "\n",
    "# for i in pesi_static.keys():\n",
    "    \n",
    "#     w = pesi_static[i]\n",
    "#     pesi = [w*1/len(ret_dict[i].columns)]*len(ret_dict[i].columns)\n",
    "#     kmeans_w = kmeans_w.append( pd.DataFrame(pesi) )\n",
    "#     names_assets = names_assets.union( ret_dict[i].columns )\n",
    "\n",
    "# kmeans_w.index = names_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excess return from daily rebalancing\n",
    "extraret_bkm = bkm_rol - bkm_static\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(extraret_bkm)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Excess Return\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between extrareturns from daily rebalancing\n",
    "extraret_difference = extraret_bkm - extraret_km\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(extraret_difference)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Excess Return\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical risk parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrp clusters (pyhrp package)\n",
    "cov, cor = ret_train.cov(), ret_train.corr()\n",
    "links = linkage(dist(cor.values), method='ward')\n",
    "node = tree(links)\n",
    "\n",
    "#rootcluster = _hrp(node, cov)\n",
    "#weights_hrp = rootcluster.weights\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax = dendrogram(links, orientation=\"top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static portfolio optimization HRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static portfolio weights (pypfopt package)\n",
    "hrp = hierarchical_portfolio.HRPOpt(ret_train)\n",
    "raw_weights = hrp.optimize()\n",
    "weights_hrp = hrp.clean_weights()\n",
    "# cluster = hrp.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "hrp_rets_static = (ret_test*list(weights_hrp.values())).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "hrp_static = cumulative_returns(pf_returns = hrp_rets_static, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(hrp_static)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.legend (['Static portfolio optimization HRP'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling portfolio optimization HRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hrp_pesirol = rolling_portfolio(ret_test, clust_type = \"hierarchical\")\n",
    "\n",
    "#hrp_pesirol.to_pickle(\"./pesi_rolling_hrp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rolling hrp weights\n",
    "hrp_pesirol = pd.read_pickle(\"./pesi_rolling_hrp.pkl\")\n",
    "\n",
    "hrp_pesirol.columns = ret_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "hrp_rets_rol = (ret_test*hrp_pesirol).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "hrp_rol = cumulative_returns(pf_returns = hrp_rets_rol, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(hrp_rol)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.legend (['Rolling portfolio optimization HRP'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excess return from daily rebalancing\n",
    "extraret = hrp_rol - hrp_static\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(extraret)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Excess Return\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling tangency portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tangency_rol = rolling_portfolio(ret_test, clust_type = \"none\")\n",
    "\n",
    "#tangency_rol.to_pickle(\"./pesi_rolling_tangency.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rolling tangency weights\n",
    "tan_pesirol = pd.read_pickle(\"./pesi_rolling_tangency.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvxpy.installed_solvers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "tan_rets_rol = (ret_test*tan_pesirol).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "tan_rol = cumulative_returns(pf_returns = tan_rets_rol, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(tan_rol)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.legend (['Rolling tangency portfolio'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static tangency portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio weights\n",
    "cluster_price = expected_returns.prices_from_returns(ret_test)\n",
    "mu = expected_returns.mean_historical_return(cluster_price)\n",
    "S = risk_models.sample_cov(cluster_price)\n",
    "ef = EfficientFrontier(mu, S)\n",
    "raw_weights = ef.max_sharpe()\n",
    "tan_pesi_static = ef.clean_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "tan_rets_rol = (ret_test*tan_pesi_static).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "tan_static = cumulative_returns(pf_returns = tan_rets_rol, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(tan_static)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.legend (['Static tangency portfolio'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excess return from daily rebalancing\n",
    "extraret = tan_rol - tan_static\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(extraret)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Excess Return\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equally weighted portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_rets = ret_test.mean(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "ew_rol = cumulative_returns(pf_returns = ew_rets, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "ax1.plot(ew_rol)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Portfolio Cumulative Returns\")\n",
    "plt.legend (['Equally wighted portfolio'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
